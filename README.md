# gcp-dataflow-etl-pipeline

A fully orchestrated ETL pipeline built with **Apache Airflow**, using **Google Cloud Dataflow** (Apache Beam) for transformation and **BigQuery** for storage. This project is designed to be deployed directly into an Airflow DAGs folder and executed via the Airflow web UI or scheduler.

---

## üìñ Description

This project implements a data pipeline for extracting, transforming, and loading (ETL) data using Google Cloud services. The pipeline logic is written using Apache Beam and executed through **Google Cloud Dataflow**, while orchestration is handled completely within **Apache Airflow**.

Once deployed into your Airflow environment, this pipeline can be triggered manually or scheduled to run periodically.

### Key Features

- ‚òÅÔ∏è Cloud-native architecture: Airflow + Dataflow + BigQuery
- üîÑ Supports batch and streaming data ingestion (e.g., Datastream)
- üß™ Beam-based transformation logic
- üóìÔ∏è Fully managed and scheduled by Apache Airflow
- üìä Final output written to BigQuery for analytics

---

## üìÅ Project Structure

```text
project-root/
‚îú‚îÄ‚îÄ dataflow_script.py     # Beam pipeline code (transform and load)
‚îú‚îÄ‚îÄ Datastream.py          # Custom source extractor or wrapper
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Requirements

### Python Packages

Install the required dependencies from `requirements.txt`:

```bash
pip install -r requirements.txt
```

Contents:
```
apache-beam[gcp]
google-cloud-bigquery
google-cloud-storage
```

### Google Cloud Setup

Make sure the following GCP APIs are enabled:
- Dataflow
- BigQuery
- Cloud Storage

And ensure your service account or user has appropriate IAM roles:
- Dataflow Admin
- BigQuery Data Editor
- Storage Admin

---

## üöÄ Deployment Steps

1. Copy the following files into your Airflow DAGs folder:
   - `dataflow_script.py`
   - `Datastream.py`
   - `requirements.txt`

2. Open the Airflow UI and confirm the DAG appears.

3. Trigger the DAG manually or configure a schedule interval.

> ‚ö†Ô∏è Note: DAG execution will launch a Dataflow job on GCP, so ensure all GCP configurations and credentials are properly set up in your Airflow environment.


